{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+qQCg3tqmkXl8UQINPooz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/AIVA-4x500m/blob/main/base/generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5yawr9g4s07",
        "outputId": "049ca34e-0a55-4c28-8741-edce11a1576b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFpSJKeE42ym",
        "outputId": "9005df4c-2c28-4fe1-a5ec-81ec1ff4c712"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10\n",
        "block_size = 256\n",
        "max_iters = 2500\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-5\n",
        "eval_iters = 250\n",
        "d_model = 512\n",
        "n_head = 18\n",
        "n_layers = 12\n",
        "dropout = 0.2\n",
        "norm_eps = 1e-05\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float = 1e-6):\n",
        "    \"\"\"\n",
        "      Initialize the RMSNorm normalization layer.\n",
        "      Args:\n",
        "        dim (int): The dimension of the input tensor.\n",
        "        eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
        "      Attributes:\n",
        "        eps (float): A small value added to the denominator for numerical stability.\n",
        "        weight (nn.Parameter): Learnable scaling parameter.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self, x):\n",
        "    \"\"\"\n",
        "      Apply the RMSNorm normalization to the input tensor.\n",
        "        Args:\n",
        "        x (torch.Tensor): The input tensor.\n",
        "      Returns:\n",
        "        torch.Tensor: The normalized tensor.\n",
        "    \"\"\"\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      Forward pass through the RMSNorm layer.\n",
        "      Args:\n",
        "          x (torch.Tensor): The input tensor.\n",
        "      Returns:\n",
        "          torch.Tensor: The output tensor after applying RMSNorm.\n",
        "    \"\"\"\n",
        "    output = self._norm(x.float()).type_as(x)\n",
        "    return output * self.weight\n",
        "\n",
        "class UnMaskedHead(nn.Module):\n",
        "  def __init__(self, head_size, d_model, block_size, dropout):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(d_model, head_size, bias=True)\n",
        "    self.query = nn.Linear(d_model, head_size, bias=True)\n",
        "    self.value = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rel_pos_embd = nn.Parameter(torch.randn(block_size, block_size, head_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** -0.5)\n",
        "    rel_pos_scores = torch.einsum('btc,tvc->btv', query, self.rel_pos_embd[:T, :T])\n",
        "    scores = scores + rel_pos_scores\n",
        "\n",
        "    att_mat = F.softmax(scores, dim=-1)\n",
        "    att_mat = self.dropout(att_mat)\n",
        "    value = self.value(x)\n",
        "    output = torch.matmul(att_mat, value)\n",
        "    return output\n",
        "\n",
        "class UnMaskedAttention(nn.Module):\n",
        "  def __init__(self, d_model, block_size, dropout, n_head):\n",
        "    head_size = d_model // n_head\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([UnMaskedHead(d_model=d_model, dropout=dropout, block_size=block_size, head_size=head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_head * head_size, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class MaskedHead(nn.Module):\n",
        "  def __init__(self, d_model, head_size, dropout, block_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.query = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.value = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** -0.5)\n",
        "    scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "    att_mat = F.softmax(scores, dim=-1)\n",
        "    att_mat = self.dropout(att_mat)\n",
        "    value = self.value(x)\n",
        "    output = torch.matmul(att_mat, value)\n",
        "    return output\n",
        "\n",
        "class CasualMaskedAttention(nn.Module):\n",
        "  def __init__(self, d_model, block_size, dropout, n_head):\n",
        "    head_size = d_model // n_head\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([MaskedHead(d_model=d_model, dropout=dropout, block_size=block_size, head_size=head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_head * head_size, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FinalHead(nn.Module):\n",
        "  def __init__(self, d_model, head_size, dropout, block_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.query = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.value = nn.Linear(d_model, head_size, bias=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, att):\n",
        "    B, T, C = x.shape\n",
        "    key = self.key(att)\n",
        "    query = self.query(att)\n",
        "\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** -0.5)\n",
        "\n",
        "    att_mat = F.softmax(scores, dim=-1)\n",
        "    att_mat = self.dropout(att_mat)\n",
        "    value = self.value(x)\n",
        "    output = torch.matmul(att_mat, value)\n",
        "    return output\n",
        "\n",
        "class FinalAttention(nn.Module):\n",
        "  def __init__(self, d_model, block_size, dropout, n_head):\n",
        "    head_size = d_model // n_head\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([FinalHead(d_model=d_model, dropout=dropout, block_size=block_size, head_size=head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_head * head_size, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, att):\n",
        "    out = torch.cat([h(x, att) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, dropout):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(d_model, 4*d_model),\n",
        "      nn.GELU(),\n",
        "      nn.Linear(4*d_model, d_model),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class EncoderNetwork(nn.Module):\n",
        "  def __init__(self, d_model, n_head, norm_eps, dropout, block_size):\n",
        "    super().__init__()\n",
        "    self.s_att = UnMaskedAttention(n_head=n_head, d_model=d_model, dropout=dropout, block_size=block_size)\n",
        "    self.ffwd = FeedForward(d_model, dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = RMSNorm(d_model, eps=norm_eps)\n",
        "\n",
        "  def forward(self, src):\n",
        "    src = self.norm(src)\n",
        "    src_out = src + self.dropout(self.s_att(src))\n",
        "\n",
        "    src = self.norm(src_out)\n",
        "    src_f = src + self.dropout(self.ffwd(src))\n",
        "\n",
        "    del src_out, src\n",
        "    return src_f\n",
        "\n",
        "class DecoderNetwork(nn.Module):\n",
        "  def __init__(self, d_model, n_head, norm_eps, dropout, block_size):\n",
        "    super().__init__()\n",
        "    self.m_att = CasualMaskedAttention(n_head=n_head, d_model=d_model, dropout=dropout, block_size=block_size)\n",
        "    self.f_att = FinalAttention(d_model=d_model, n_head=n_head, dropout=dropout, block_size=block_size)\n",
        "    self.ffwd = FeedForward(d_model, dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = RMSNorm(d_model, eps=norm_eps)\n",
        "\n",
        "  def forward(self, src, att):\n",
        "    m_att_out = self.norm(src)\n",
        "    m_out = src + self.dropout(self.m_att(m_att_out))\n",
        "\n",
        "    f_out = self.f_att(m_out, self.norm(att))\n",
        "    f_out = m_out + self.dropout(f_out)\n",
        "\n",
        "    src_f = self.norm(f_out)\n",
        "    src_f = f_out + self.dropout(self.ffwd(src_f))\n",
        "\n",
        "    del f_out, m_out, m_att_out, src, att\n",
        "    return src_f\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.block_size = block_size\n",
        "    self.toked_model = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encod = nn.Embedding(block_size, d_model)\n",
        "    self.enc_layer = nn.ModuleList([EncoderNetwork(n_head=n_head, norm_eps=norm_eps, block_size=block_size, dropout=dropout, d_model=d_model) for _ in range(n_layers)])\n",
        "    self.dec_layer = nn.ModuleList([DecoderNetwork(n_head=n_head, norm_eps=norm_eps, block_size=block_size, dropout=dropout, d_model=d_model) for _ in range(n_layers)])\n",
        "    self.norm_final = RMSNorm(d_model, eps=norm_eps)\n",
        "    self.linear_final = nn.Linear(d_model, vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    \"\"\"\n",
        "      initialize weights of linear and embedding layers\n",
        "\n",
        "      Args:\n",
        "        - module (nn.Module): the module to initialize weights for\n",
        "    \"\"\"\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias.data)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"\n",
        "      forward pass of the transformer model\n",
        "\n",
        "    Args:\n",
        "      - idx (Tensor): input tensor representing token indices\n",
        "      - targets (Tensor): target tensor for computing loss during training\n",
        "\n",
        "    Returns:\n",
        "      - logits (Tensor): output logits from the final linear layer\n",
        "      - loss (Tensor): optional. computed cross-entropy loss if targets are provided, else None\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    toked_model = self.toked_model(idx)\n",
        "    pos_encod = self.pos_encod(torch.arange(T, device=device))\n",
        "    x = toked_model + pos_encod\n",
        "\n",
        "    for layer in self.enc_layer:\n",
        "      x_out = layer(x)\n",
        "\n",
        "    for layer in self.dec_layer:\n",
        "      x_final = layer(x, x_out)\n",
        "\n",
        "    x_final = self.dropout(x_final)\n",
        "    x_final = self.norm_final(x_final)\n",
        "    logits = self.linear_final(x_final)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -self.block_size:]\n",
        "        # get the predictions\n",
        "        logits, loss = self(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "dQP-Ap4i4-t4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
        "tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
        "\n",
        "vocab_size = tokenizer.n_vocab\n",
        "model = Transformer(vocab_size)\n",
        "checkpoint_path = '/content/drive/MyDrive/base-500m.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "osvwSp845FI2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"why is he like that and\"\n",
        "seed_tokens = tokenizer.encode(seed)\n",
        "seed_tokens = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated = m.generate(seed_tokens, max_new_tokens=10)\n",
        "generated_text = tokenizer.decode(generated[0].tolist())\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZScMKpWa76_h",
        "outputId": "205a3706-3d25-48df-e3e6-551ba55e0e8d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why is he like that and is he he he he is he is that he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def generate(model, input_ids, max_length=100, temperature=1.0, top_k=0, top_p=0.0, beam_search=False, beam_size=3):\n",
        "    \"\"\"\n",
        "    Generate text using the provided model.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The Transformer model to use for generation.\n",
        "    - tokenizer: Tokenizer object to convert tokens to text.\n",
        "    - seed_text (str): The initial text to start generation.\n",
        "    - max_length (int): Maximum length of the generated sequence.\n",
        "    - temperature (float): Temperature scaling parameter for softmax.\n",
        "    - top_k (int): Number of top tokens to sample from using top-k sampling.\n",
        "    - top_p (float): Nucleus (top-p) sampling parameter.\n",
        "    - beam_search (bool): Whether to use beam search for generation.\n",
        "    - beam_size (int): Size of the beam for beam search.\n",
        "\n",
        "    Returns:\n",
        "    - generated_text (str): The generated text sequence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        current_length = input_ids.size(1)\n",
        "\n",
        "        while current_length < max_length:\n",
        "            logits = model(input_ids)[0][:, -1, :]  # Get logits for the last token\n",
        "            logits /= temperature  # Apply temperature scaling\n",
        "            filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "            probabilities = F.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "            if beam_search is True:\n",
        "                # Beam search\n",
        "                beam_scores, beam_indices = torch.topk(probabilities, k=beam_size, dim=-1)\n",
        "                input_ids = torch.cat([input_ids[:, :, None].expand(-1, -1, beam_size), beam_indices.unsqueeze(1)], dim=2)\n",
        "                current_length += 1\n",
        "            else:\n",
        "                # Sampling\n",
        "                next_token = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "                current_length += 1\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\"\n",
        "    Apply top-k and top-p (nucleus) filtering to logits.\n",
        "\n",
        "    Args:\n",
        "    - logits (torch.Tensor): Logits tensor of shape (batch_size, vocab_size).\n",
        "    - top_k (int): Number of top tokens to keep.\n",
        "    - top_p (float): Cumulative probability threshold for nucleus sampling.\n",
        "    - filter_value (float): Value to fill filtered logits outside the top-k and top-p.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_logits (torch.Tensor): Filtered logits tensor.\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        logits = top_k_filtering(logits, top_k=top_k, filter_value=filter_value)\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        logits = nucleus_sampling(logits, top_p=top_p)\n",
        "\n",
        "    return logits\n",
        "\n",
        "def top_k_filtering(logits, top_k=0, filter_value=-float('Inf')):\n",
        "    \"\"\"\n",
        "    Apply top-k filtering to logits.\n",
        "\n",
        "    Args:\n",
        "    - logits (torch.Tensor): Logits tensor of shape (batch_size, vocab_size).\n",
        "    - top_k (int): Number of top tokens to keep.\n",
        "    - filter_value (float): Value to fill filtered logits outside the top-k.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_logits (torch.Tensor): Filtered logits tensor.\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        values, indices = logits.topk(top_k, dim=-1)\n",
        "        min_values = values[:, -1].unsqueeze(-1).repeat(1, logits.size(-1))\n",
        "        logits = torch.where(logits < min_values, torch.ones_like(logits) * filter_value, logits)\n",
        "    return logits\n",
        "\n",
        "def nucleus_sampling(logits, top_p=0.0):\n",
        "    \"\"\"\n",
        "    Apply nucleus (top-p) sampling to logits.\n",
        "\n",
        "    Args:\n",
        "    - logits (torch.Tensor): Logits tensor of shape (batch_size, vocab_size).\n",
        "    - top_p (float): Cumulative probability threshold for nucleus sampling.\n",
        "\n",
        "    Returns:\n",
        "    - sampled_logits (torch.Tensor): Sampled logits tensor.\n",
        "    \"\"\"\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    # Remove tokens with cumulative probability > top_p\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    # Set logits of removed tokens to a large negative value\n",
        "    sampled_logits = sorted_logits.clone()\n",
        "    sampled_logits[sorted_indices_to_remove] = -float('Inf')\n",
        "\n",
        "    return sampled_logits"
      ],
      "metadata": {
        "id": "_J7toP6u5IcI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the model\n",
        "seed_text = \"Once upon a time\"\n",
        "input_tokens = tokenizer.encode(seed_text)\n",
        "print(input_tokens)\n",
        "\n",
        "input_ids = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_text = generate(model, input_ids, max_length=50, temperature=1, top_k=20, top_p=0)\n",
        "generated_text = tokenizer.decode(generated_text[0].tolist())\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKKTS-mQ66l4",
        "outputId": "b2a98278-c36c-4570-a41b-5abf8d9372f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7454, 2402, 257, 640]\n",
            "Generated Text:\n",
            "Once upon a time learning upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon upon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decoder(logits, k=3, max_length=128, length_penalty=0.6):\n",
        "    sequences = [[[100], 0.0]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            input_ids = seq\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, none)\n",
        "                next_token_logits = outputs[0][:, -1, :]\n",
        "                log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            topk_log_probs, topk_tokens = torch.topk(log_probs, k, dim=-1)\n",
        "            topk_log_probs = topk_log_probs.cpu().numpy()[0]\n",
        "            topk_tokens = topk_tokens.cpu().numpy()[0]\n",
        "\n",
        "            for log_prob, token in zip(topk_log_probs, topk_tokens):\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score - log_prob\n",
        "                all_candidates.append([new_seq, new_score])\n",
        "\n",
        "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
        "        sequences = ordered[:k]\n",
        "\n",
        "    return sequences[0][0]\n",
        "\n",
        "def generate_output(input_ids, model, max_length=50, beam_width=3, temperature=1.0, top_p=0.9):\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(input_ids)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        logits /= temperature\n",
        "\n",
        "        # Apply nucleus sampling (top-p sampling)\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[:, indices_to_remove] = float('-inf')\n",
        "\n",
        "        # Beam search decoding\n",
        "        generated_tokens = beam_search_decoder(logits, k=beam_width, max_length=max_length)\n",
        "    return generated_tokens\n",
        "\n",
        "# Example usage\n",
        "input_tokens = \"Once upon a time\"\n",
        "context = torch.tensor([tokenizer.encode(input_tokens)], dtype=torch.long, device=device)\n",
        "output_tokens = generate_output(context, model)\n",
        "output_text = tokenizer.decode(output_tokens)\n",
        "print(output_tokens, '\\n' ,output_text)"
      ],
      "metadata": {
        "id": "-fRIDCb-5LUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}